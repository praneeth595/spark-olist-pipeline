{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61db1ea5-e30a-42f1-87f0-1be18d7b58c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.functions import broadcast, to_date\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "GOLD_PATH = \"/Volumes/spark_olist/gold/delta/daily_sales_state\"\n",
    "\n",
    "def load_silver(spark):\n",
    "    orders    = spark.read.format(\"delta\").load(\"/Volumes/spark_olist/silver/delta/orders_clean\")\n",
    "    customers = spark.read.format(\"delta\").load(\"/Volumes/spark_olist/silver/delta/customers_clean\")\n",
    "    payments  = spark.read.format(\"delta\").load(\"/Volumes/spark_olist/silver/delta/order_payments_clean\")\n",
    "    return orders, customers, payments\n",
    "\n",
    "def build_daily_state_revenue(orders, customers, payments):\n",
    "    pay_per_order = payments.groupBy(\"order_id\").agg(F.sum(\"payment_value\").alias(\"payment_total\"))\n",
    "    daily = (\n",
    "        orders.join(broadcast(customers), \"customer_id\", \"left\")\n",
    "              .join(pay_per_order, \"order_id\", \"inner\")\n",
    "              .withColumn(\"order_date\", to_date(\"order_purchase_timestamp\"))\n",
    "              .groupBy(\"order_date\", \"customer_state\")\n",
    "              .agg(F.sum(\"payment_total\").alias(\"daily_revenue\"))\n",
    "    )\n",
    "    return daily\n",
    "\n",
    "def initial_write(daily_df):\n",
    "    (daily_df.write.format(\"delta\")\n",
    "            .partitionBy(\"order_date\")\n",
    "            .option(\"maxRecordsPerFile\", 50_000)\n",
    "            .mode(\"overwrite\")\n",
    "            .save(GOLD_PATH))\n",
    "\n",
    "def merge_incremental(spark):\n",
    "    \n",
    "    target = DeltaTable.forPath(spark, GOLD_PATH)\n",
    "\n",
    "    \n",
    "    max_date_row = spark.read.format(\"delta\").load(GOLD_PATH)\\\n",
    "                      .agg(F.max(\"order_date\").alias(\"max_date\")).collect()[0]\n",
    "    max_date = max_date_row[\"max_date\"]\n",
    "\n",
    "    orders, customers, payments = load_silver(spark)\n",
    "    pay_per_order = payments.groupBy(\"order_id\").agg(F.sum(\"payment_value\").alias(\"payment_total\"))\n",
    "\n",
    "    incr = (\n",
    "        orders.filter(F.col(\"order_purchase_timestamp\") > F.to_timestamp(F.lit(max_date)))\n",
    "              .join(customers, \"customer_id\", \"left\")\n",
    "              .join(pay_per_order, \"order_id\", \"inner\")\n",
    "              .withColumn(\"order_date\", to_date(\"order_purchase_timestamp\"))\n",
    "              .groupBy(\"order_date\", \"customer_state\")\n",
    "              .agg(F.sum(\"payment_total\").alias(\"daily_revenue\"))\n",
    "    )\n",
    "\n",
    "    (target.alias(\"t\")\n",
    "           .merge(incr.alias(\"s\"),\n",
    "                  \"t.order_date = s.order_date AND t.customer_state = s.customer_state\")\n",
    "           .whenMatchedUpdate(set={\"daily_revenue\": \"s.daily_revenue\"})\n",
    "           .whenNotMatchedInsert(values={\n",
    "               \"order_date\": \"s.order_date\",\n",
    "               \"customer_state\": \"s.customer_state\",\n",
    "               \"daily_revenue\": \"s.daily_revenue\"\n",
    "           }).execute())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48c6122c-a837-4703-9d2d-3de20f03d3aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "    spark = (SparkSession.builder.appName(\"Olist Gold Daily Sales\")\n",
    "             .config(\"spark.sql.adaptive.enabled\", \"true\").getOrCreate())\n",
    "    orders, customers, payments = load_silver(spark)\n",
    "    daily = build_daily_state_revenue(orders, customers, payments)\n",
    "    initial_write(daily)           # run once\n",
    "    # merge_incremental(spark)     # use for subsequent runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70d6f91b-007f-4d60-9e1a-0eab00430710",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "gold_sales.py",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
