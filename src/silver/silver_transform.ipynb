{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f543f26-2580-45cb-baec-c999c364470a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run\n",
    "/Workspace/Repos/h20240186@pilani.bits-pilani.ac.in/spark-olist-pipeline/src/utils/config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3521e0c-fa47-4a73-b53a-7a785af2c305",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, trim, lower, upper, initcap\n",
    "def curate_to_silver(key,value):\n",
    "\n",
    "    df=spark.read.format(\"delta\").load(value[\"input_path\"])\n",
    "\n",
    "    if value[\"deduplicate_cols\"]:\n",
    "        df=df.dropDuplicates(value[\"deduplicate_cols\"])\n",
    "\n",
    "    if value[\"standardize_cols\"]:\n",
    "        for col_name, rule in value[\"standardize_cols\"].items():\n",
    "            if rule == \"lower\":\n",
    "                df = df.withColumn(col_name, lower(trim(col(col_name))))\n",
    "            elif rule == \"upper\":\n",
    "                df = df.withColumn(col_name, upper(trim(col(col_name))))\n",
    "            elif rule == \"title\":\n",
    "                df = df.withColumn(col_name, initcap(trim(col(col_name))))\n",
    "\n",
    "    \n",
    "    df.write.format(\"delta\")\\\n",
    "        .mode(\"overwrite\")\\\n",
    "        .option(\"maxRecordsPerFile\", 1000000)\\\n",
    "        .saveAsTable(key)\n",
    "    print(f\"Table {key} created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29bb88a3-96c1-4585-960a-4b359414c2c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for key,value in silver_sources.items():\n",
    "    curate_to_silver(key,value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cf9f50e-63cc-4912-a758-8db85ed934e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Broadcasting small tables\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "products    = spark.table(\"products_clean\")                     \n",
    "translations= spark.table(\"product_category_translation_clean\") \n",
    "order_items = spark.table(\"order_items_clean\")                  \n",
    "sellers     = spark.table(\"sellers_clean\")  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dim_products = (\n",
    "    products.join(broadcast(translations),\"product_category_name\",\"left\")\n",
    ")\n",
    "\n",
    "order_items_enriched = (\n",
    "    order_items.join(broadcast(dim_products), \"product_id\", \"left\")\n",
    ")\n",
    "order_items_enriched = (\n",
    "    order_items_enriched.join(broadcast(sellers), \"seller_id\", \"left\")\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04ad56d8-4c84-4bbc-9055-54372665f457",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "buckets = 8  \n",
    "salts = spark.range(buckets).withColumnRenamed(\"id\", \"__salt\")\n",
    "\n",
    "\n",
    "order_items = spark.table(\"order_items_clean\").select(\n",
    "    \"order_id\",\"product_id\",\"seller_id\",\"price\",\"freight_value\"\n",
    ")\n",
    "\n",
    "\n",
    "sellers = spark.table(\"sellers_clean\").select(\"seller_id\",\"seller_city\",\"seller_state\")\n",
    "sellers_salted = sellers.crossJoin(salts)  \n",
    "\n",
    "\n",
    "oi_salted = order_items.withColumn(\n",
    "    \"__salt\",\n",
    "    F.pmod(F.xxhash64(F.col(\"order_id\")), F.lit(buckets)).cast(\"int\")\n",
    ")\n",
    "\n",
    "\n",
    "order_items_enriched = (\n",
    "    oi_salted.join(sellers_salted, [\"seller_id\",\"__salt\"], \"left\")\n",
    "             .drop(\"__salt\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c80267a-383e-4a88-8130-92c9d3fb56fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "silver_transform",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
